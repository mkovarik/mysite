# Probability Theory I

## Data generating processes

In the last few lectures, we used descriptive statistics to
describe and summarize data. But *where* does data come from?

Statisticians use the phrase **data generating process** to describe
the procedure that produces an observed data set. For example, the
process of conducting a political survey is a data generating
process. Performing a chemical experiment is a data generating
process. Rolling dice is a data generating process. 

**Inferential statistics** is the discipline of statistics responsible
for drawing conclusions about a data generating process using the
observed data set (the **sample**).

**Probability theory**, the mathematics of chance, forms the theoretical
foundation of inferential statistics. Data generating processes are
formalized as **probabalistic models**, which takes into account random
effects in data generation. Statistical inference procedures require
such models to formally contextualize observed data.

### Example: flipping coins

For example, suppose one is interested in determining if a coin is
unbalanced (the likelihood of flipping a heads is different than
that of flipping a tails). Intuitively, if one were to flip the
coin 100 times (the data generating process), one should obtain
approximately 50 heads and 50 tails (the resultant sample).
However, due to randomness, this will likely not be the case even
if the coin is balanced.

Suppose the following result was observed:

\\[\begin{aligned}
\text{number of heads} &= 58 \\\\
\text{number of tails} &= 42
\end{aligned}\\]

Can we conclude that the coin is biased? This question can actually
be easily answered with only one line of code (in the
[R programming language][rwik]):


```R
> prop.test(x = 58, n = 100, p = 0.5)

	1-sample proportions test with continuity correction

data:  58 out of 100, null probability 0.5
X-squared = 2.25, df = 1, p-value = 0.1336
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
 0.4771046 0.6766764
sample estimates:
   p 
0.58
```

Notice that the output contains the equation `p-value = 0.1336`.
The **p-value** of an experiment is a statistic that can be
thought of as a "score" for a given hypothesis (in this case,
the hypothesis that the coin is unbalanced). Low p-values
(by convention, < 0.05) indicate **statistical significance**,
and that the hypothesis in question can be regarded as true.

In the coin-flipping example, a p-value of 0.1336 would typically be
considered too large to conclude that the coin is unbalanced.

## Sampling procedures

A **population** is an unobserved data set that is of interest in a
statistical experiment. Sometimes, the word population is used
somewhat synonymously with "data generating process". But this
terminology can be confusing.

Suppose we wish to determine the average wage of all American
workers. This would require summing all their annual wages and dividing
the result by the number of Americans workers.

However, there are hundreds of millions of American workers. So it is
infeasable to collect all that wage data (unless we were a govenment
agency). So instead, a **sample** (observed subset of a population) is
collected. The sample mean may then serve as a useful estimate of the
population mean.

The validity of the above inference depends on two things: (1) the
**sample size** (number of observations in the sample) should be
sufficiently large, and (2) the **sampling procedure** (method
to which samples are collected) is specified and is representative
of the population under consideration.

The most common type of sampling procedure is the **simple random
sample** (SRS). In this model, all possible samples are equally likely.
There are two variants of simple random sampling:

* **SRS without replacements**: Here, a given item from a population
  can be sampled at most one time. Think of this as "moving" an item
  from a population to a sample.
* **SRS with replacements:** Here, a given item from a population
  can be sampled multiple times. Think of this as "copying" an item
  from a population to a sample. This type of sampling yields
  **independent observations**.

**[Note** There are many inferential procedures in the course text
that list SRS without replacement as an assumption. This is technically
false. A more theoretically correct assumption is that observations
be independent (SRS with replacements). However, as long as the sample
size is small relative to the population size (perhaps < 20% or so),
then SRS without replacement produces *practically independent* observations.
**]]**


There are other types of sampling procedures, they include:

* Systematic sampling
* Stratefied sampling
* Cluster sampling

Read section 1.3 of the course text for more information.

## Sampling bias

Regardless of the sampling procedure, it is important that the sample
be representative of the population in question. If that is not the
case, then there may be **selection bias** when conducting statistical
inference.

For example, the [1936 US presidential election][preswiki] pitted
Democratic President Franklin Roosevelt against Alf Landon, the Republican
governor from Kansas. This was the first presidential election for which
polling was widely employed. And if polls were indicative of the general
political mood, Landon was to win with a landslide.

But Roosevelt won 46 of the 48 states. It was an electoral slaughter.

The reason why the polls were so off is that they were conducted via
telephone. At the time, telephones were luxuries that were enjoyed by the
relatively wealthy, who tend to vote Republican, a pro-business party. The sample
thus contained a higher proportion of Republicans than the voting population.
Sampling bias got in the way of correct political forecasting.


[rwik]: https://en.wikipedia.org/wiki/R_(programming_language) 
[preswiki]: https://en.wikipedia.org/wiki/United_States_presidential_election,_1936 