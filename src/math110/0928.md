# Probability Theory IV


### Conditional probabilities

Probabilities are subjective. The probabilities we assign to events is
relative to the information that we have access to. Mathematically, this
notion is codified by the concept **conditional probabilities**.

The probability of an event \\(E_1\\) conditioned on an event
\\(E_2\\) is denoted as \\(P(E_1 | E_2)\\). This is the probability of
\\(E_1\\) occuring given that we know \\(E_2\\) has already occurred.

It is useful to think of \\(P(E_1)\\) as the probability of as the "prior"
probability of the event \\(E_1\\). It is the probability of \\(E_1\\)
calculated prior to us learning that \\(E_2\\) occurred. Then
\\(P(E_1 | E_2) \\) can be thought of as the "posterior" probability of
\\(E_1\\) calculated after we learned of the occurrance of \\(E_2\\).

Bayesian statisticians make liberal use of distinguishing "prior" and
"posterior" probabilities. As a Bayesian statistician accumulates data,
their previous calibration of probabilistic models (given by a
"prior distribution") is altered (changed tothe "posterior
distribution").

Frequentist statistcians also make heavy use of conditional
probabilities. For example, the notorious p-value is a statistic that
is defined as a conditional probability.

The following formula can be used to calculate conditional
probabilities:

\\[P(E_1 | E_2) = \frac{P(E_1 \text{ \\& } E_2)}{P(E_2)}. \\]

### Example: medical tests

Let \\(D\\) be the event that you have some [disease][pokerus]. To see
that you have the disease, a medical test is performed. The results
are either positive or negative, indicating that either you have the d
isease or that you do not have the disease.

Such tests are fallible. And so we may have the following
two-way table of relative frequencies/probabilities:

```
         | D      | not D   | TOTAL |
         +--------+---------+-------+
Positive |  0.009 | 0.198   | 0.207 |
Negative |  0.001 | 0.792   | 0.793 | 
---------+--------+---------+-------+
TOTAL    |  0.01  | 0.99    | 1

```


**Problem.** What is the probability that you have the disease prior
to getting tested?

**Solution.** 1%. ■

**Problem.** How reliable is the test?

**Solution.** If you have the disease, the
probability of testing positive is

\\[\begin{aligned}
P(\text{pos } | D) &= \frac{P(\text{pos \\& } D)}{P(D)} \\\\
&= \frac{0.009}{0.01} \\\\
&= 90\\%.
\end{aligned}\\]

Similarly, the probability of testing negative if you don't have the disease is
80%. ■

**Problem.** Suppose you took the test and you tested positive. How likely is
it do you have the disease?

**Solution.** Not very likely.

\\[\begin{aligned}
P(D | \text{pos })
&= \frac{P(D \text{ \\& pos})}{P(\text{pos})} \\\\
&= \frac{0.009}{0.207} \\\\
&\approx 4.35\\%
\end{aligned}\\] 

This result is highly unintuitive. ■

## Probabilities of intersections

The intersection of two events can be calculated using conditional
probabilities:

\\[P(E_1 \text{ \\& } E_2) = P(E_1 | E_2) P(E_2). \\]

## Independence

The following four statements are equivalent for events \\(E_1\\)
and \\(E_2\\) with nonzero probabilities:

1. \\(E_1\\) and \\(E_2\\) are **independent**.
2. \\(P(E_1 | E_2) = P(E_1)\\).
3. \\(P(E_2 | E_1) = P(E_2)\\).
4. \\(P(E_1 \text{ \\& } E_2) = P(E_1) P(E_2)\\).

Often, the fourth statement is used as the definition of independence
since it allows for either \\(E_1\\) or \\(E_2\\) to have zero
probability (which is sometimes the case with infinitely large outcome
spaces).

The second and third statement, however, provide an intuitive
understanding of independence. Think of the process of updating the
prior probabilities to the posterior probabilities as gaining
information. In statements two and three, the prior and posterior
probabilities are the same. So knowing one event gives no information
about the other event if the two events are independent.

**Example.** Sampling with replacement produces independent
observations. Sampling without replacement does not (observations are
**dependent**). Suppose, for example, that each value in the population
is unique. When sampling with replacement, you know that no two
observations can have the same value. So knowing one observation provides
some information about the other observations. The observations are not
independent. ■


**[[Note:** A lot of students mix up the *independence* with *mutually
exclusive*. This is very wrong. In fact, two independent events can never
be mutually exclusive (at least for finite outcome spaces). **]]**

In some cases, independence can be determined by the provided formulas.
However, in most cases, independence must be assumed or inferred from the
context of the problem. For example, in rolling two dice,

The concept of independence can be extended to three or more events. If
\\(E_1\\), \\(E_2\\), and \\(E_3\\) are independent, then

\\[P(E_1 \text{ \\& } E_2 \text{ \\& } E_3) = P(E_1) P(E_2) P(E_3).\\]

Note that the converse of this is not true. To prove independence between
three events, one must also show that they are pairwise independent:

* \\(P(E_1 \text{ \\& } E_2) = P(E_1) P(E_2)\\)
* \\(P(E_2 \text{ \\& } E_3) = P(E_2) P(E_3)\\)
* \\(P(E_3 \text{ \\& } E_1) = P(E_3) P(E_1)\\)

[pokerus]: https://bulbapedia.bulbagarden.net/wiki/Pok%C3%A9Rus